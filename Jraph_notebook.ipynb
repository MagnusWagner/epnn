{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcb48b1",
   "metadata": {},
   "source": [
    "# Testing jraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d2e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1bffe0715f4193b445bbaf776adbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocessing_jraph import get_init_crystal_states\n",
    "# import ase.db\n",
    "import jax\n",
    "import typing\n",
    "# from preprocessing import get_cutoff_mask, get_init_charges, get_gaussian_distance_encodings, v_center_at_atoms_diagonal, type_to_charges_dict, SYMBOL_MAP\n",
    "from jax import lax, random, numpy as jnp\n",
    "import optax\n",
    "import jraph\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple\n",
    "\n",
    "\n",
    "key, subkey = random.split(random.PRNGKey(0))\n",
    "h_dim = 126\n",
    "e_dim = 48\n",
    "layers = [32,32,1] # hidden layers\n",
    "T = 3\n",
    "path = \"data/SrTiO3_500.db\"\n",
    "n_elems = 3\n",
    "R_SWITCH = 0.5\n",
    "R_CUT = 3.0\n",
    "\n",
    "\n",
    "descriptors, distances, distances_encoded, init_charges, gt_charges, cutoff_mask = get_init_crystal_states(path, edge_encoding_dim = e_dim, SAMPLE_SIZE = None, r_switch = 0.5, r_cut = 5.0) # Change sample size to None if all samples should be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e443deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_implicitly_batched_graphsTuple_with_encoded_distances(descriptors, distances, distances_encoded, init_charges, cutoff_mask, cutoff = R_CUT):\n",
    "    batch_size = descriptors.shape[0]\n",
    "    natom = descriptors.shape[1]\n",
    "    # Reshaping the descriptors to go over the whole batch\n",
    "    descriptors = jnp.reshape(descriptors,(batch_size*natom,descriptors.shape[2]))\n",
    "    \n",
    "    # to calculate the number of edges for each individual graph\n",
    "    distances_flattened_batchwise = jnp.reshape(distances,(batch_size,natom*natom))\n",
    "    n_edges = jnp.count_nonzero(jnp.logical_and(distances_flattened_batchwise > 0, distances_flattened_batchwise < cutoff),axis=1)\n",
    "    n_nodes = jnp.repeat(jnp.array([natom]),batch_size)\n",
    "    # Create a flattened index over all previously diagonal elements to be able to delete them from the flattened arrays.\n",
    "    flatten_idx = jnp.nonzero(jnp.logical_and(distances.flatten() > 0, distances.flatten() < cutoff))[0]\n",
    "    idx = jnp.nonzero(jnp.logical_and(distances.flatten() > 0, distances.flatten() < cutoff))[0]\n",
    "    # Make sure that there are only edges between nodes of the same graph\n",
    "    # Batch range to add onto the tiled outer products\n",
    "    batch_range = jnp.reshape(jnp.repeat(jnp.arange(batch_size)*natom,natom*natom),(batch_size,natom,natom))\n",
    "    # outer product over the atoms\n",
    "    outer = jnp.tile(jnp.outer(jnp.ones(natom),jnp.arange(natom)).astype(jnp.int32),batch_size).reshape(batch_size,natom,natom)\n",
    "    # transposed for the other variant\n",
    "    outer_transposed = jnp.transpose(outer, axes=(0,2,1))\n",
    "    senders = jnp.add(outer_transposed,batch_range).flatten()[flatten_idx]\n",
    "    receivers = jnp.add(outer,batch_range).flatten()[flatten_idx]\n",
    "    sender_descriptors = descriptors[senders,:]\n",
    "    receiver_descriptors = descriptors[senders,:]\n",
    "    # Encoded distances are also flattened. Combinations of the same node (diagonal) are deleted\n",
    "    graph_edges = jnp.reshape(distances_encoded,(distances_encoded.shape[0]*distances_encoded.shape[1]*distances_encoded.shape[2],48))[flatten_idx,:]\n",
    "    # Same for cutoff_mask\n",
    "    cutoff_mask = cutoff_mask.flatten()[flatten_idx]\n",
    "    # Nodes contain charges\n",
    "    # Edges contain concatenation of descriptors, edge_embeddings and cutoff_mask (which will be removed in the Network)\n",
    "    graph= jraph.GraphsTuple(nodes = init_charges.flatten(),\n",
    "                            # nodes = jnp.concatenate([descriptors,init_charges],axis=-1), Alternative \n",
    "                            senders = senders,\n",
    "                            receivers = receivers,\n",
    "                            edges = jnp.concatenate([receiver_descriptors, sender_descriptors, graph_edges, jnp.expand_dims(cutoff_mask,axis=-1)],axis=-1),\n",
    "                            n_node = n_nodes,\n",
    "                            n_edge = n_edges,\n",
    "                            globals = None)\n",
    "    return graph\n",
    "\n",
    "size = 100\n",
    "graph = create_implicitly_batched_graphsTuple_with_encoded_distances(descriptors[:size],distances[:size], distances_encoded[:size],init_charges[:size],cutoff_mask[:size])\n",
    "graph2 = create_implicitly_batched_graphsTuple_with_encoded_distances(descriptors[size:size+100],distances[size:size+100], distances_encoded[size:size+100],init_charges[size:size+100],cutoff_mask[size:size+100])\n",
    "# graph = create_implicitly_batched_graphsTuple_with_encoded_distances(descriptors,distances, distances_encoded,init_charges,cutoff_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14520dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes-Shape: (10500,)\n",
      "Edges-Shape: (95044, 301)\n",
      "Senders-Length: (95044,)\n",
      "Globals: None\n",
      "n_nodes: [105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105 105\n",
      " 105 105 105 105 105 105 105 105 105 105]\n",
      "n_edges: [958 942 926 954 942 950 950 966 974 958 970 934 962 946 942 958 930 958\n",
      " 954 950 954 958 946 938 986 958 946 942 930 954 930 950 966 974 958 958\n",
      " 922 962 958 934 934 938 950 966 946 938 926 954 970 946 946 946 950 934\n",
      " 942 942 946 986 954 942 958 938 982 962 946 962 918 966 982 962 942 966\n",
      " 946 946 958 926 946 934 966 926 938 930 930 954 942 934 938 950 958 998\n",
      " 922 950 930 934 974 974 946 970 974 962]\n"
     ]
    }
   ],
   "source": [
    "def print_graph_stats(graph):\n",
    "    print(\"Nodes-Shape:\",graph[0].shape)\n",
    "    print(\"Edges-Shape:\",graph[1].shape)\n",
    "    print(\"Senders-Length:\",graph[2].shape)\n",
    "    print(\"Globals:\",None if graph[4] is None else graph[4].shape)\n",
    "    print(\"n_nodes:\",graph[5])\n",
    "    print(\"n_edges:\",graph[6])\n",
    "\n",
    "print_graph_stats(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb52e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5713183 1.5674556\n",
      "1.9611565 1.9656975\n",
      "1.0207137 1.0227226\n",
      "0.26732582 0.26901242\n",
      "0.15774243 0.16158329\n",
      "0.11951331 0.12304594\n",
      "0.1018314 0.10459877\n",
      "0.08950808 0.09216454\n",
      "0.080596566 0.083806045\n",
      "0.07386086 0.077766344\n",
      "0.06826447 0.072916836\n"
     ]
    }
   ],
   "source": [
    "import jax.tree_util as tree\n",
    "import haiku as hk\n",
    "\n",
    "def aggregate_edges_for_nodes_fn(edges: jnp.array,\n",
    "                                receivers: jnp.array,\n",
    "                                cutoff_mask: jnp.array,\n",
    "                                n_nodes: int) -> jnp.array:\n",
    "  edges = jnp.multiply(edges,cutoff_mask)\n",
    "  return jax.ops.segment_sum(edges,receivers,n_nodes)\n",
    "\n",
    "\n",
    "class MLP_haiku(hk.Module):\n",
    "  def __init__(self, features: jnp.ndarray):\n",
    "    super().__init__()\n",
    "    self.features = features\n",
    "\n",
    "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    layers = []\n",
    "    for feat in self.features[:-1]:\n",
    "      layers.append(hk.Linear(feat))\n",
    "      layers.append(jax.nn.relu)\n",
    "    layers.append(hk.Linear(self.features[-1]))\n",
    "    mlp = hk.Sequential(layers)\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphElectronPassing(aggregate_edges_for_nodes_fn: Callable,\n",
    "                        MLP: Callable,\n",
    "                        h_dim: int = 126) -> Callable:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_edges_for_nodes_fn: function used to aggregates the sender nodes.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGEP(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    receiver_descriptors = edges[:,:h_dim]\n",
    "    sender_descriptors=edges[:,h_dim:h_dim*2]\n",
    "    graph_edges = edges[:,h_dim*2:-1]\n",
    "    cutoff_mask = jnp.expand_dims(edges[:,-1],axis=-1)\n",
    "    sender_charges = jnp.expand_dims(nodes[senders],axis=-1)\n",
    "    receiver_charges = jnp.expand_dims(nodes[receivers],axis=-1)\n",
    "    # Neural network forward: NN(q_v, q_w, h_v, h_w, e_vw) from the paper\n",
    "    edges = jnp.concatenate([receiver_charges, sender_charges, receiver_descriptors, sender_descriptors, edges],axis=-1)\n",
    "    edges_reversed = jnp.concatenate([sender_charges, receiver_charges, sender_descriptors, receiver_descriptors, edges],axis=-1)\n",
    "    # Subtraction of both outputs to create electron-passing-output for atom v\n",
    "    MLP_outputs = jnp.subtract(MLP(edges),MLP(edges_reversed))\n",
    "    # aggregate_edges_for_nodes_fn is the weighting function with the cutoff_mask\n",
    "    received_attributes = tree.tree_map(\n",
    "      lambda e: aggregate_edges_for_nodes_fn(e, receivers, cutoff_mask, nodes.shape[0]), MLP_outputs)\n",
    "    nodes = received_attributes.flatten()\n",
    "    return graph._replace(nodes=nodes)\n",
    "  return _ApplyGEP\n",
    "\n",
    "\n",
    "gep_layer = GraphElectronPassing(\n",
    "    aggregate_edges_for_nodes_fn=aggregate_edges_for_nodes_fn,\n",
    "    MLP = lambda n: MLP_haiku(features=[32,32,32, 1])(n),\n",
    ")\n",
    "NUM_PASSES = 2\n",
    "model = hk.without_apply_rng(hk.transform(gep_layer))\n",
    "params = model.init(jax.random.PRNGKey(42), graph)\n",
    "true_labels = gt_charges[:size].flatten()\n",
    "true_labels_val = gt_charges[size:size+100].flatten()\n",
    "opt_init, opt_update = optax.adam(1e-2)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "out_graph = model.apply(params,graph)\n",
    "# out_graph = model.apply(params,out_graph)\n",
    "# out_graph = model.apply(params,out_graph)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def rmse_loss(params: hk.Params, graph: jraph.GraphsTuple,  ground_truth: jnp.array, num: int = 2) -> jnp.ndarray:\n",
    "    # hk.fori_loop(0,3, model.apply, graph, params=params)\n",
    "    output = model.apply(params, graph)\n",
    "    for i in range(num-1):\n",
    "      output = model.apply(params, output)\n",
    "    # output = model.apply(params, output)\n",
    "    # output = model.apply(params, output)\n",
    "    return jnp.sqrt(jnp.sum(jnp.square(output[0]-ground_truth)/len(ground_truth)))\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "    \"\"\"Returns updated params and state.\"\"\"\n",
    "    g = jax.grad(rmse_loss)(params, graph, true_labels)\n",
    "    updates, opt_state = opt_update(g, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "print(rmse_loss(params, graph, true_labels),rmse_loss(params, graph2, true_labels_val)) \n",
    "for i in range(100):\n",
    "  params, opt_state = update(params, opt_state)\n",
    "  if (i % 10)==0: \n",
    "    print(rmse_loss(params, graph,true_labels),rmse_loss(params, graph2, true_labels_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2838e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_graphsTuple_with_encoded_distances(descriptors, distances, distances_encoded, init_charges, cutoff_mask, cutoff = R_CUT):\n",
    "#     natom = descriptors.shape[0]\n",
    "#     # Create a flattened index over all previously diagonal elements to be able to delete them from the flattened arrays. \n",
    "#     flatten_idx = jnp.nonzero(jnp.logical_and(distances > 0, distances < cutoff).flatten())[0]\n",
    "#     senders = jnp.outer(jnp.ones(natom),jnp.arange(natom)).T.flatten()[flatten_idx].astype(jnp.int32)\n",
    "#     receivers = jnp.outer(jnp.ones(natom),jnp.arange(natom)).flatten()[flatten_idx].astype(jnp.int32)\n",
    "#     sender_descriptors = descriptors[senders,:]\n",
    "#     # print(senders,receivers)\n",
    "#     receiver_descriptors = descriptors[senders,:]\n",
    "#     n_nodes = jnp.array([natom])\n",
    "#     n_edges = jnp.array([natom*natom-natom])\n",
    "#     # Encoded distances are also flattened. Combinations of the same node (diagonal) are deleted\n",
    "#     graph_edges = jnp.reshape(distances_encoded,(distances_encoded.shape[0]*distances_encoded.shape[1],48))[flatten_idx,:]\n",
    "#     # Same for cutoff_mask\n",
    "#     cutoff_mask = cutoff_mask.flatten()[flatten_idx]\n",
    "#     # Nodes contain charges\n",
    "#     # Edges contain concatenation of descriptors, edge_embeddings and cutoff_mask (which will be removed in the Network)\n",
    "#     graph= jraph.GraphsTuple(nodes = init_charges,\n",
    "#                             # nodes = jnp.concatenate([descriptors,init_charges],axis=-1), Alternative \n",
    "#                             senders = senders,\n",
    "#                             receivers = receivers,\n",
    "#                             edges = jnp.concatenate([receiver_descriptors, sender_descriptors, graph_edges, jnp.expand_dims(cutoff_mask,axis=-1)],axis=-1),\n",
    "#                             n_node = n_nodes,\n",
    "#                             n_edge = n_edges,\n",
    "#                             globals = None)\n",
    "#     return graph\n",
    "\n",
    "# graph = create_graphsTuple_with_encoded_distances(descriptors[0],distances[0], distances_encoded[0],init_charges[0],cutoff_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f102626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# natom = 4\n",
    "# batch_size = 2\n",
    "\n",
    "# batch_range = jnp.reshape(jnp.repeat(jnp.arange(batch_size)*natom,natom*natom),(batch_size,natom,natom))\n",
    "# outer = jnp.tile(jnp.outer(jnp.ones(natom),jnp.arange(natom)).astype(jnp.int32),batch_size).reshape(batch_size,natom,natom)\n",
    "# outer_transposed = jnp.transpose(outer, axes=(0,2,1))\n",
    "# senders = jnp.add(outer_transposed,batch_range).flatten()\n",
    "# receivers = jnp.add(outer,batch_range).flatten()\n",
    "\n",
    "# senders, receivers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
